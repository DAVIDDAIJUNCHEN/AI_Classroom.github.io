<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>序列区分性训练</title>
    <url>/2021/11/23/paper-0.0.3/chapters/kaldi%20tutorial/</url>
    <content><![CDATA[<p><h1 style="text-align: center"> Kaldi 示例 </h1></p>
<div style="text-align: center"><small>陈代君 - 2021</small></div>




<h2 id="1-Kaldi-简介"><a href="#1-Kaldi-简介" class="headerlink" title="1. Kaldi 简介"></a>1. Kaldi 简介</h2><h3 id="1-1-名字来历"><a href="#1-1-名字来历" class="headerlink" title="1.1 名字来历"></a>1.1 名字来历</h3><p>Kaldi 是埃塞俄比亚的牧羊人，发现了咖啡树。因为 Kaldi 最初诞生于 2009 年约翰霍普金斯大学的夏季研讨会，参与研讨会的研究人员当中多数都是咖啡的爱好者，于是 Kaldi 的创作者使用发现咖啡树的人名字命名该语音解码器。后来这个解码器继续开发成了现在的 Kaldi 语音识别工具。</p>
<h3 id="1-2-发展历程"><a href="#1-2-发展历程" class="headerlink" title="1.2 发展历程"></a>1.2 发展历程</h3><ul>
<li>2009 年约翰霍普金斯大学夏季研讨会，仅含有轻量级解码器和 HTK 训练脚本。</li>
<li>2010 年的 Kaldi 研讨会，完善了语音识别工具包功能。</li>
<li>2011 年 正式发布 Kaldi 的代码库，同时在同年的 Kaldi 研讨会，基于GMM 和 SGMM 区分性训练脚本被开发。</li>
<li>2012 年 Kaldi 研讨会， nnet1 的区分性训练和 stacked-bottleneck 网络。</li>
<li>2013 年 Kaldi 研讨会补充完善 Kaldi。</li>
<li>2014 年 JSALT 研讨会，讨论了神经网络内部结构和语音识别的置信度分析。</li>
<li>2015年开始 nnet3 的开发。</li>
</ul>
<h3 id="1-3-Kaldi-特点"><a href="#1-3-Kaldi-特点" class="headerlink" title="1.3 Kaldi 特点"></a>1.3 Kaldi 特点</h3><ul>
<li>最开始由于开发人员多使用过 HTK, 同时很多开发者都来自于剑桥大学，比如 Daniel Povey, 所以最开始 Kaldi 和 HTK 很类似。</li>
<li>底层源码由 C++ 编写，上层代码由 Python, Perl, shell 组成。</li>
<li>总是包含最新的语音技术。</li>
<li>开源，采用了 Apache License Version 2.0。即可以利用 Kaldi 进行商用。</li>
<li>缺点，文档不全。所以通过给予示例的形式来分享成果。</li>
</ul>
<h2 id="2-Yes-No-例子"><a href="#2-Yes-No-例子" class="headerlink" title="2. Yes/No 例子"></a>2. Yes/No 例子</h2><p>省略安装步骤，可以直接在CRG 上面调用配置好的 docker 环境，进入到编译好的 kaldi 环境中。</p>
<p><code>alias kaldi=&#39;/asr/build/container/lmtools/mkl/shell.sh&#39;</code></p>
<p>Yes/No 例子是 Kaldi 当中最简单的一个示例，但是却包含了通常 ASR 模型构建的主干内容。所以学习了这个案例之后，对于 ASR 的基本流程会有一个较为清晰的认识。</p>
<h3 id="2-1-运行脚本"><a href="#2-1-运行脚本" class="headerlink" title="2.1 运行脚本"></a>2.1 运行脚本</h3><p><p align = "center"><br>    <img height="600px" src = "./images/YesNoStruct.png"><br>    图 2-1. YesNo 案例的文件结构</p>
<p>Kaldi 的所有示例的主脚本是最外层文件夹下面的 run.sh, 其余的脚本和文件都在主脚本运行的过程中被调用。</p>
<p><code>./run.sh</code></p>
<h3 id="2-2-运行结果"><a href="#2-2-运行结果" class="headerlink" title="2.2 运行结果"></a>2.2 运行结果</h3><p>运行该脚本仅需要非常短的时间，大概 2~3 分钟之后便会得到如下所示的结果。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">local</span>/score.sh: scoring with word insertion penalty=0.0,0.5,1.0</span><br><span class="line">exp/mono0a/decode_test_yesno/wer_9_0.5:%WER 0.00 [ 0 / 232, 0 ins, 0 del, 0 sub ]</span><br><span class="line">exp/mono0a/decode_test_yesno/wer_9_1.0:%WER 0.00 [ 0 / 232, 0 ins, 0 del, 0 sub ]</span><br><span class="line">%WER 0.00 [ 0 / 232, 0 ins, 0 del, 0 sub ] exp/mono0a/decode_test_yesno/wer_10_0.0</span><br></pre></td></tr></table></figure>
<p>上面的内容显示在解码过程中使用了0.0，0.5，1.0 的词嵌入惩罚之后，得到的 WER 为 0，[ 0 / 232, 0 ins, 0 del, 0 sub ] 表示在 232 个测试单词上有 0 个错误，0 个错误中有 0 个是 insertion (插入错误)，0 个 deletion (删除错误)， 0 个 substitution (替换错误)。最后一行表示最好的测试结果出现在解码参数为 (10, 0) 组合条件下。</p>
<h3 id="2-3-主脚本层面解读"><a href="#2-3-主脚本层面解读" class="headerlink" title="2.3 主脚本层面解读"></a>2.3 主脚本层面解读</h3><p>主脚本非常简单，总共包含了 49 行代码。</p>
<h4 id="2-3-1-训练-解码命令定义"><a href="#2-3-1-训练-解码命令定义" class="headerlink" title="2.3.1 训练/解码命令定义"></a>2.3.1 训练/解码命令定义</h4><p>在代码的最开始部分，定义了使用何种方式来训练和解码。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"></span><br><span class="line">train_cmd=<span class="string">&quot;utils/run.pl&quot;</span></span><br><span class="line">decode_cmd=<span class="string">&quot;utils/run.pl&quot;</span></span><br><span class="line"><span class="comment">#train_cmd=&quot;utils/slurm.pl&quot;</span></span><br><span class="line"><span class="comment">#decode_cmd=&quot;utils/slurm.pl&quot;</span></span><br></pre></td></tr></table></figure>
<p>上面的 两个变量 train_cmd 和 decode_cmd 分别定义了使用 utils/run.pl 来多任务的执行程序。执行如下命令行，</p>
<blockquote>
<p>utils/run.pl JOB=1:3 ./log.JOB echo “run job JOB”</p>
</blockquote>
<p>如上的命令执行了三个 echo 命令，并把屏幕上的标准输出写到 ./log.1-3 三个文件当中。其中的 log.1 日志文件如下，记录了实际命令，开始时间，屏幕输出，结束时间。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># echo &quot;run job 1&quot;</span></span><br><span class="line"><span class="comment"># Started at Tue Nov 23 20:54:47 EST 2021</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">run job 1</span><br><span class="line"><span class="comment"># Accounting: time=0 threads=1</span></span><br><span class="line"><span class="comment"># Ended (code 0) at Tue Nov 23 20:54:47 EST 2021, elapsed time 0 seconds</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>kaldi 的主脚本 run.sh，在模型训练和解码的过程中，分别使用了上面介绍的两个变量，</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Mono training</span></span><br><span class="line">steps/train_mono.sh --nj 1 --cmd <span class="string">&quot;<span class="variable">$train_cmd</span>&quot;</span> \</span><br><span class="line">  --totgauss 400 \</span><br><span class="line">  data/train_yesno data/lang exp/mono0a </span><br><span class="line">  </span><br><span class="line"><span class="comment"># Decoding</span></span><br><span class="line">steps/decode.sh --nj 1 --cmd <span class="string">&quot;<span class="variable">$decode_cmd</span>&quot;</span> \</span><br><span class="line">    exp/mono0a/graph_tgpr data/test_yesno exp/mono0a/decode_test_yesno</span><br></pre></td></tr></table></figure>
<h4 id="2-3-2-数据获取"><a href="#2-3-2-数据获取" class="headerlink" title="2.3.2 数据获取"></a>2.3.2 数据获取</h4><p>完成了定义训练/解码模型的方式，接下来开始下载和解压数据</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> [ ! -d waves_yesno ]; <span class="keyword">then</span></span><br><span class="line">  wget http://www.openslr.org/resources/1/waves_yesno.tar.gz || <span class="built_in">exit</span> 1;</span><br><span class="line">  <span class="comment"># was:</span></span><br><span class="line">  <span class="comment"># wget http://sourceforge.net/projects/kaldi/files/waves_yesno.tar.gz || exit 1;</span></span><br><span class="line">  tar -xvzf waves_yesno.tar.gz || <span class="built_in">exit</span> 1;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">train_yesno=train_yesno</span><br><span class="line">test_base_name=test_yesno</span><br></pre></td></tr></table></figure>
<p>上面代码首先从 OpenSLR 网站上下载名为 waves_yesno.tar.gz 的文件， 然后解包到 waves_yesno 路径。该压缩文件中包含了所有建模过程中所使用到的训练及测试数据, 一共是 60 个如下 wav 格式音频数据。</p>
<p><p align = "center"><br>    <img height="90px" src = "./images/YesNoWav.png"><br>    图 2-2. waves_yesno 目录案例</p>
<p>图 2-2 中展示了解压之后文件夹 waves_yesno 下部分音频文件的文件名。这些文件均包含了 8 个以 1/0/_ 进行命名的文件，其中 1 表示 Yes, 0 表示 No, 使用下划线进行连接来表示该语音片段所包含内容的标签。</p>
<p>通常来说，除了需要准备音频文件外，还需提供与音频文件对齐的标注文本 (transcription)，通过id来进行对齐。但是yes/no 示例中，直接使用音频对应的标注作为文件名。</p>
<h4 id="2-3-3-数据整理"><a href="#2-3-3-数据整理" class="headerlink" title="2.3.3 数据整理"></a>2.3.3 数据整理</h4><ol>
<li><p><strong>训练和测试数据准备</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Data preparation</span><br><span class="line">local/prepare_data.sh waves_yesno</span><br></pre></td></tr></table></figure>
<p>执行上面的命令行后，会得到两个数据集，data/train_yesno 和 data/test_yesno,</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">train_yesno/</span><br><span class="line">├── spk2utt</span><br><span class="line">├── text</span><br><span class="line">├── utt2dur</span><br><span class="line">├── utt2spk</span><br><span class="line">└── wav.scp</span><br><span class="line"></span><br><span class="line">dev_yesno/</span><br><span class="line">├── spk2utt</span><br><span class="line">├── text</span><br><span class="line">├── utt2dur</span><br><span class="line">├── utt2spk</span><br><span class="line">└── wav.scp</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>人工准备资源</strong></p>
<p>在 input 下面需要一些人工准备的资源，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">input/</span><br><span class="line">├── lexicon_nosil.txt</span><br><span class="line">├── lexicon.txt</span><br><span class="line">├── phones.txt</span><br><span class="line">└── task.arpabo</span><br></pre></td></tr></table></figure>
<ul>
<li><p>lexicon.txt</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">YES Y</span><br><span class="line">NO N</span><br></pre></td></tr></table></figure>
</li>
<li><p>lexicon_nosil.txt</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;SIL&gt; SIL</span><br><span class="line">YES Y</span><br><span class="line">NO N</span><br></pre></td></tr></table></figure>
</li>
<li><p>phones.txt</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SIL</span><br><span class="line">Y</span><br><span class="line">N</span><br></pre></td></tr></table></figure>
</li>
<li><p>task.arpabo</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">\data\</span><br><span class="line">ngram 1=4</span><br><span class="line"></span><br><span class="line">\1-grams:</span><br><span class="line">-1      NO</span><br><span class="line">-1      YES</span><br><span class="line">-99 &lt;s&gt;</span><br><span class="line">-1 &lt;/s&gt;</span><br><span class="line"></span><br><span class="line">\end\</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>准备语言文件夹</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">local</span>/prepare_dict.sh</span><br><span class="line">utils/prepare_lang.sh --position-dependent-phones <span class="literal">false</span> data/<span class="built_in">local</span>/dict <span class="string">&quot;&lt;SIL&gt;&quot;</span> data/<span class="built_in">local</span>/lang data/lang</span><br><span class="line"><span class="built_in">local</span>/prepare_lm.sh</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;finished &quot;</span></span><br></pre></td></tr></table></figure>
<p>前两行准备 data/lang （语言文件夹）。第三行脚本准备 lm，将 arpa 格式的 LM 转换成 WFST 的格式。</p>
</li>
</ol>
<h4 id="2-3-4-提取声学特征"><a href="#2-3-4-提取声学特征" class="headerlink" title="2.3.4 提取声学特征"></a>2.3.4 提取声学特征</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Feature extraction</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> train_yesno test_yesno; <span class="keyword">do</span> </span><br><span class="line"> steps/make_mfcc.sh --nj 1 data/<span class="variable">$x</span> exp/make_mfcc/<span class="variable">$x</span> mfcc</span><br><span class="line"> steps/compute_cmvn_stats.sh data/<span class="variable">$x</span> exp/make_mfcc/<span class="variable">$x</span> mfcc</span><br><span class="line"> utils/fix_data_dir.sh data/<span class="variable">$x</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>执行了如上脚本之后，可以到 data/train_yesno 以及 data/dev_yesno 两个数据集对应的 feats.scp 文件，其中 data/train_yesno/feats.scp 文件如下，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0_0_0_0_1_1_1_1 /mnt/users/daijun_chen/gits/github/kaldi/egs/yesno/s5/mfcc/raw_mfcc_train_yesno.1.ark:16</span><br><span class="line">0_0_0_1_0_0_0_1 /mnt/users/daijun_chen/gits/github/kaldi/egs/yesno/s5/mfcc/raw_mfcc_train_yesno.1.ark:8386</span><br></pre></td></tr></table></figure>
<h4 id="2-3-5-声学模型的训练"><a href="#2-3-5-声学模型的训练" class="headerlink" title="2.3.5 声学模型的训练"></a>2.3.5 声学模型的训练</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Mono training</span></span><br><span class="line">steps/train_mono.sh --nj 1 --cmd <span class="string">&quot;<span class="variable">$train_cmd</span>&quot;</span> \</span><br><span class="line">  --totgauss 400 \</span><br><span class="line">  data/train_yesno data/lang exp/mono0a </span><br></pre></td></tr></table></figure>
<h4 id="2-3-6-构建状态图-HCLG"><a href="#2-3-6-构建状态图-HCLG" class="headerlink" title="2.3.6 构建状态图 HCLG"></a>2.3.6 构建状态图 HCLG</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Graph compilation  </span></span><br><span class="line">utils/mkgraph.sh data/lang_test_tg exp/mono0a exp/mono0a/graph_tgpr</span><br></pre></td></tr></table></figure>
<h4 id="2-3-7-使用解码器进行解码"><a href="#2-3-7-使用解码器进行解码" class="headerlink" title="2.3.7 使用解码器进行解码"></a>2.3.7 使用解码器进行解码</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Decoding</span></span><br><span class="line">steps/decode.sh --nj 1 --cmd <span class="string">&quot;<span class="variable">$decode_cmd</span>&quot;</span> \</span><br><span class="line">    exp/mono0a/graph_tgpr data/test_yesno exp/mono0a/decode_test_yesno</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> exp/*/decode*; <span class="keyword">do</span> [ -d <span class="variable">$x</span> ] &amp;&amp; grep WER <span class="variable">$x</span>/wer_*; <span class="keyword">done</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> exp/*/decode*; <span class="keyword">do</span> [ -d <span class="variable">$x</span> ] &amp;&amp; grep WER <span class="variable">$x</span>/wer_* | utils/best_wer.sh; <span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>输出的声学模型在路径 exp/mono0a 下面，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mono0a/</span><br><span class="line">├── 0.mdl</span><br><span class="line">├── 40.mdl</span><br><span class="line">├── 40.occs</span><br><span class="line">├── ali.1.gz</span><br><span class="line">├── cmvn_opts</span><br><span class="line">├── decode_test_yesno</span><br><span class="line">│   ├── lat.1.gz</span><br><span class="line">│   ├── log</span><br><span class="line">│   ├── num_jobs</span><br><span class="line">│   ├── scoring_kaldi</span><br><span class="line">│   │   ├── best_wer</span><br><span class="line">│   │   ├── log</span><br><span class="line">│   │   ├── penalty_0.0</span><br><span class="line">│   │   │   ├── *.txt</span><br><span class="line">│   │   │   └── log</span><br><span class="line">│   │   ├── penalty_0.5</span><br><span class="line">│   │   │   ├── *.txt</span><br><span class="line">│   │   │   └── log</span><br><span class="line">│   │   ├── penalty_1.0</span><br><span class="line">│   │   │   ├── *.txt</span><br><span class="line">│   │   │   └── log</span><br><span class="line">│   │   ├── test_filt.txt</span><br><span class="line">│   │   └── wer_details</span><br><span class="line">│   │       ├── lmwt</span><br><span class="line">│   │       ├── ops</span><br><span class="line">│   │       ├── per_spk</span><br><span class="line">│   │       ├── per_utt</span><br><span class="line">│   │       ├── wer_bootci</span><br><span class="line">│   │       └── wip</span><br><span class="line">│   ├── wer_10_*.*</span><br><span class="line">├── final.mdl -&gt; 40.mdl</span><br><span class="line">├── final.occs -&gt; 40.occs</span><br><span class="line">├── fsts.1.gz</span><br><span class="line">├── graph_tgpr</span><br><span class="line">│   ├── disambig_tid.int</span><br><span class="line">│   ├── HCLG.fst</span><br><span class="line">│   ├── num_pdfs</span><br><span class="line">│   ├── phones</span><br><span class="line">│   │   ├── align_lexicon.int</span><br><span class="line">│   │   ├── align_lexicon.txt</span><br><span class="line">│   │   ├── disambig.int</span><br><span class="line">│   │   ├── disambig.txt</span><br><span class="line">│   │   ├── optional_silence.csl</span><br><span class="line">│   │   ├── optional_silence.int</span><br><span class="line">│   │   ├── optional_silence.txt</span><br><span class="line">│   │   └── silence.csl</span><br><span class="line">│   ├── phones.txt</span><br><span class="line">│   └── words.txt</span><br><span class="line">├── log</span><br><span class="line">├── num_jobs</span><br><span class="line">├── phones.txt</span><br><span class="line">└── tree</span><br></pre></td></tr></table></figure>
<h4 id="2-3-8-识别结果样例"><a href="#2-3-8-识别结果样例" class="headerlink" title="2.3.8 识别结果样例"></a>2.3.8 识别结果样例</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">1_0_0_0_0_0_0_0 YES NO NO NO NO NO NO NO</span><br><span class="line">1_0_0_0_0_0_0_1 YES NO NO NO NO NO NO YES</span><br><span class="line">1_0_0_0_0_0_1_1 YES NO NO NO NO NO YES YES</span><br><span class="line">1_0_0_0_1_0_0_1 YES NO NO NO YES NO NO YES</span><br><span class="line">1_0_0_1_0_1_1_1 YES NO NO YES NO YES YES YES</span><br><span class="line">1_0_1_0_1_0_0_1 YES NO YES NO YES NO NO YES</span><br><span class="line">1_0_1_1_0_1_1_1 YES NO YES YES NO YES YES YES</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>语音识别</tag>
      </tags>
  </entry>
  <entry>
    <title>序列区分性训练</title>
    <url>/2021/10/09/paper-0.0.3/chapters/discriminative%20training/</url>
    <content><![CDATA[<p><h1 style="text-align: center"> 序列区分性训练 </h1></p>
<div style="text-align: center"><small>陈代君 - 2021</small></div>




<h2 id="1-生成式模型的缺陷"><a href="#1-生成式模型的缺陷" class="headerlink" title="1. 生成式模型的缺陷"></a>1. 生成式模型的缺陷</h2><p>​    语音识别的本质可以用下面的公式进行描述，</p>
<script type="math/tex; mode=display">
\max_{S}\mathbb{P}(S|O)</script><p>其中 <script type="math/tex">O</script> 表示观测到的语音信号，<script type="math/tex">S</script> 表示词序列。传统的语音识别进一步分解上面的公式，</p>
<script type="math/tex; mode=display">
\max_{S} \mathbb{P}(S|O) = \max_{S}\frac{\mathbb{P}_{\theta}(O|S)\mathbb{P}(S)}{\mathbb{P}(O)}</script><p>其中，分母 <script type="math/tex">\mathbb{P}(O)</script> 在声音信号给定之后是不需要考虑的常量；分子上 <script type="math/tex">\mathbb{P}(S)</script> 是语言模型， <script type="math/tex">\mathbb{P}_{\theta}(O|S)</script> 是声学模型。 声学模型通常是基于给定的 <script type="math/tex">N</script> 个训练数据 <script type="math/tex">\{( S_{i}, O_{i})\}_{i=1}^{N}</script> 求得 <script type="math/tex">\theta</script> 的极大似然估计。 </p>
<p>​    为了能够理解极大似然估计的缺陷，先回顾一下极大似然估计。 统计的频率学派将参数 <script type="math/tex">\theta</script> 视为未知的常量，在模型类型给定的基础上，基于训练数据寻找使得似然最大的 <script type="math/tex">\theta</script>，记为 <script type="math/tex">\hat{\theta}_{MLE}</script>， 这个值就是 <script type="math/tex">\theta</script> 的极大似然估计。求极大似然估计的过程中，首先需要给定模型类型，例如在语音识别中，通常假定模型为 GMM-HMM 或者 DNN-HMM。因为真实的语音信号并不是由 HMM 生成的，所以在 GMM-HMM 模型的基础上根据似然最大准则找到的最优参数并不一定使得 WER 或者 SER 最小。 换句话说，似然最大准则并不能保证语音识别的错误率最小是极大似然估计的主要缺陷。</p>
<p>​    实际上，<script type="math/tex">\mathbb{P}_{\theta}(O|S)</script> 是描述数据分布的生成式模型 (generative model)，即给定词序列，假设观测到的语音信号服从 GMM-HMM 或者 DNN-HMM 模型描述的分布。不同于生成式模型，区分性模型 (discriminative model，也称为判别式模型) 直接对给定观测数据条件下真实类别的分布进行建模。即对 <script type="math/tex">\mathbb{P}_{\theta}(S|O)</script> 进行建模。</p>
<h2 id="2-MMI-估计"><a href="#2-MMI-估计" class="headerlink" title="2. MMI 估计"></a>2. MMI 估计</h2><h3 id="2-1-MMI-估计的基本原理"><a href="#2-1-MMI-估计的基本原理" class="headerlink" title="2.1 MMI 估计的基本原理"></a>2.1 MMI 估计的基本原理</h3><p>因为语音信号随机变量 <script type="math/tex">\mathbf{O}</script> 和词序列随机变量 <script type="math/tex">\mathbf{S}</script> 的互信息如下，</p>
<script type="math/tex; mode=display">
\begin{align}\mathbb{I}(\mathbf{O}, \mathbf{S}) &= \mathbb{E}\log\frac{\mathbb{P}(\mathbf{O},\mathbf{S})}{\mathbb{P}(\mathbf{O})\mathbb{P}(\mathbf{S})}\\
&=\mathbb{H}(\mathbf{S})-\mathbb{H}(\mathbf{S}|\mathbf{O})\end{align}</script><p>其中， <script type="math/tex">\mathbb{H}(\mathbf{S})</script> 表示词序列随机变量的熵， <script type="math/tex">\mathbb{H}(\mathbf{S}|\mathbf{O})</script> 表示条件为观测随机变量的条件熵。</p>
<p>​    同时 <script type="math/tex">\mathbb{H}(\mathbf{S})</script> 与待估计模型的参数无关。 所以极大化互信息 <script type="math/tex">\mathbb{I}(\mathbf{O}, \mathbf{S})</script> 等价于极小化条件熵 <script type="math/tex">\mathbb{H}(\mathbf{S}|\mathbf{O})</script>。极小化条件熵意味着在给定观测语音信号随机变量 <script type="math/tex">\mathbf{O}</script> 后，极小化词序列随机变量 <script type="math/tex">\mathbf{S}</script> 的不确定性。 又因为互信息表达式， </p>
<script type="math/tex; mode=display">
\mathbb{I}(\mathbf{O}，\mathbf{S}) = \mathbb{E}\log\frac{\mathbb{P}(\mathbf{O},\mathbf{S})}{\mathbb{P}(\mathbf{O})} - \mathbb{E}\log\mathbb{P}(\mathbf{S})</script><p>其中 <script type="math/tex">\mathbb{P}(\mathbf{S})</script> 不含模型待估参数。 因此，极大化互信息等价于极大化上式右边第一项。所以极大互信息 (MMI, maximum mutual information) 直接求使得 <script type="math/tex">\sum_{i=1}^{N}\log\mathbb{P}_{\theta}(S_i|O_i)</script> 极大的 <script type="math/tex">\hat{\theta}_{MMI}</script>，这个估计被称为 MMI 估计。具体的表达式如下，</p>
<script type="math/tex; mode=display">
\begin{align}
\hat{\theta}_{MMI}  &= \arg\max_{\theta} \sum_{i=1}^{N}\log\mathbb{P}_{\theta}(S_{i}|O_{i})\\
&=\arg \max_{\theta}\sum_{i=1}^{N}\log\frac{\mathbb{P}_{\theta}(O_{i}|S_{i})\mathbb{P}(S_{i})}{\mathbb{P}_{\theta}(O_{i})}\\ &=\arg \max_{\theta}\sum_{i=1}^{N}\log\frac{\mathbb{P}_{\theta}(O_{i}|S_{i})\mathbb{P}(S_{i})}{\sum_{j}\mathbb{P}_{\theta}(O_{i}|S_{i}^{j})\mathbb{P}(S_{i}^{j})}
\end{align}</script><p>其中，<script type="math/tex">S_{i}^{j}</script> 表示给定观测语音信号 <script type="math/tex">O_{i}</script> 条件下可能的词序列。例如，当正确的语音为 “动手学语音识别”，则 “冻手雪余音识别” 可以作为其中一个可能的词序列。</p>
<p>​    为了极大化 MMI 的目标函数，可以增加分子或者减小分母 （MMI 的目标函数可以转化为似然函数乘积的形式）。 增加分子意味着提高给定正确词序列条件下观测语音信号的概率，即声学模型的概率。减小分母等同于减少错误词序列条件下观测语音的概率。综合起来看，增加正确词序列的概率减少错误词序列的概率使得最终的 MMI 估计具有更好的区分性，从而更好的识别出正确的词序列。</p>
<h3 id="2-2-MMI-训练算法推导"><a href="#2-2-MMI-训练算法推导" class="headerlink" title="2.2 MMI 训练算法推导"></a>2.2 MMI 训练算法推导</h3><h4 id="2-2-1-GMM-HMM"><a href="#2-2-1-GMM-HMM" class="headerlink" title="2.2.1 GMM-HMM"></a>2.2.1 GMM-HMM</h4><p>​    理解了 MMI 的基本原理之后，还需要能够快速寻找 MMI 目标函数的优化算法。 相对于极大似然目标函数的优化来说， MMI 的目标函数的优化是更难的。总的来说，有两类算法被用来优化 MMI 的目标函数。第一类是基于梯度的优化算法，例如 GPD (Generalized Probabilistic Descent)。 第二类是更接近 EM 算法的 EB (Extended Baum-Welch) 算法。相对于基于梯度的优化算法，EB 算法具有下面的优势。</p>
<ul>
<li>EB 算法利用辅助函数从理论上确保了算法的收敛和高效。</li>
<li>EB 算法可以处理 HMM 中每个状态到其他状态的转移概率求和等于 1 以及 GMM 各分量权重求和等于 1 这两个等式约束条件。 </li>
</ul>
<p>​    下面是 GMM-HMM 模型的 EB 算法第 <script type="math/tex">k+1</script> 步的迭代公式， 证明的细节参考附录。</p>
<script type="math/tex; mode=display">
\begin{align}
\mu_{jm}^{(k+1)} &= \frac{\theta^{num}_{jm}(\mathbf{O})-\theta_{jm}^{den}(\mathbf{O})+D_{jm}\cdot\mu_{jm}^{(k)}}{\gamma_{jm}^{num}-\gamma_{jm}^{den}+D_{jm}} \\
(\sigma^{2})^{(k+1)}_{jm} &= \frac{\theta_{jm}^{num}(\mathbf{O}^{2})-\theta_{jm}^{den}(\mathbf{O}^2)+D_{jm}\cdot\big((\sigma^{2})_{jm}^{(k)}+(\mu_{jm}^{(k)})^2\big)}{\gamma_{jm}^{num}-\gamma_{jm}^{den}+D_{jm}}-(\mu_{jm}^{(k+1)})^{2}
\end{align}</script><p>其中，</p>
<script type="math/tex; mode=display">
\begin{align}
f^{num}(\theta) &= \sum_{i=1}^{N}\log\mathbb{P}_{\theta}(O_{i}|S_{i})\mathbb{P}(S_{i}) = \sum_{i=1}^{N}\log\sum_{a_i}f^{num}_{a_i}(\theta) \\
f^{den}(\theta) &= \sum_{i=1}^{N}\log\sum_{j}\mathbb{P}_{\theta}(O_i|S_i^j)\mathbb{P}_{\theta}(S_i^j) = \sum_{i=1}^{N}\log\sum_{j}\sum_{a_{ij}}f^{den}_{a_{ij}}(\theta)
\end{align}</script><script type="math/tex; mode=display">
\begin{align}
\gamma_{ijm}^{num}(t) &= \sum_{a_{i}(t)=j}\frac{f^{num}_{a_{i}}(\theta^{(k)})}{\sum_{y}f^{num}_{y}(\theta^{(k)})},
\gamma_{ijm}^{den}(t) = \sum_{l}\sum_{a_{il}(t)=j}\frac{f_{a_{il}}^{den}(\theta^{(k)})}{\sum_{y}f_{y}^{den}(\theta^{(k)})} \\
\gamma_{jm}^{num} &= \sum_{i=1}^{N}\sum_{t=1}^{T_{i}}\gamma_{ijm}^{num}(t),
\gamma_{jm}^{den} = \sum_{i=1}^{N}\sum_{t=1}^{T_{i}}\gamma_{ijm}^{den}(t) \\ 
\theta_{jm}^{num}(\mathbf{O}) &= \sum_{i=1}^{N}\sum_{t=1}^{T_{i}}\gamma_{ijm}^{num}(t)O_i(t),
\theta_{jm}^{den}(\mathbf{O}) = \sum_{i=1}^{N}\sum_{t=1}^{T_{i}}\gamma_{ijm}^{den}(t)O_i(t)\\
\theta_{jm}^{num}(\mathbf{O}^2) &= \sum_{i=1}^{N}\sum_{t=1}^{T_{i}}\gamma_{ijm}^{num}(t)O_i^2(t), 
\theta_{jm}^{den}(\mathbf{O}^2) = \sum_{i=1}^{N}\sum_{t=1}^{T_{i}}\gamma_{ijm}^{den}(t)O^2_{i}(t)
\end{align}</script><p>上面的参数更新公式中，<script type="math/tex">\gamma_{ijm}^{den}(t)</script> 的计算需要遍历所有可能的词序列。因为所有可能的词序列数量巨大，所以直接通过上面的更新公式来求解参数的 MMI 估计几乎是不可能的。 为了克服这个计算上的困难，下面两个小节将介绍两种解决方法。</p>
<h4 id="2-2-2-DNN-HMM"><a href="#2-2-2-DNN-HMM" class="headerlink" title="2.2.2 DNN-HMM"></a>2.2.2 DNN-HMM</h4><h3 id="2-3-Lattice-based-MMI"><a href="#2-3-Lattice-based-MMI" class="headerlink" title="2.3 Lattice-based MMI"></a>2.3 Lattice-based MMI</h3><p>​    为了加速 MMI 的计算量，早期的工作主要集中在使用 N-best 列表来近似 MMI 分母中所有可能词序列的集合。但是对于非常长的词序列，LM 计算出来的概率都接近于零，导致 N-best 中存储的备选词序列不能很好地近似分母所需的词序列集合 [Chow, 1990] 。另一种逼近 MMI 分母词序列的方案中，对每一个训练词序列生成一个 Lattice（词格），该 Lattice 将会被应用到 MMI 训练且在迭代过程不会被更新 [Povey, 2003]。</p>
<p>​    Lattice-based MMI 的具体实现步骤如下，</p>
<hr>
<p><strong>实现步骤</strong></p>
<ol>
<li>分子部分 Lattice 的构造：基于极大似然准则训练好 GMM-HMM（或基于 CE 准则训练好 DNN-HMM 模型），将训练好的模型应用到正确标准的词序列和音频信号序列上进行状态级别的对齐并生成正确词序列对应的 Lattice。</li>
<li>分母部分 Lattice 的构造: 利用 unigram 语言模型构建出 HCLG, 识别所有的训练例子 <script type="math/tex">(O_i, S_i)</script> 并保留识别过程中的 Lattice, <script type="math/tex">i=1,\dots,N</script>。 尽管在 [Povey, 2003] 文章中，主要利用 bigram 语言模型构建 HCLG，但是论文也提到利用 unigram 语言模型构建 HCLG 会得到更好的实验结果。</li>
<li>Lattice-based MMI 的迭代训练: 对于一个训练样本 <script type="math/tex">(O_i, S_i)</script>，<ul>
<li>基于 1. 中构造的分子 Lattice, 利用 unigram 语言模型得到正确路径的概率并计算 <script type="math/tex">\gamma_{ijm}^{num}</script> 的前向和后向概率。</li>
<li>基于 2. 中构造的分母 Lattice, 构造正确路径对应的竞争路径集合，并利用 unigram 语言模型计算该竞争路径的概率，类似分子部分计算每条竞争路径的概率后求和得到 <script type="math/tex">\gamma_{ijm}^{den}</script> 。</li>
<li>迭代更新模型参数至收敛，得到参数的 MMI 估计。</li>
</ul>
</li>
</ol>
<hr>
<p>​    上面的实现步骤中，利用两个不同的模型分别对分子和分母部分构建分子 Lattice 以及分母 Lattice。其中分母 Lattice 的构造， 利用了由简单的 unigram 语言模型组成的 HCLG 有限状态转移图。在 MMI 的迭代更新过程中，分母和分子部分的计算都利用了 unigram 语言模型计算路径的概率。</p>
<p>​    下面的表格总结了 [Povey, 2003] 中报告的 Lattice-based MMI 训练过程中的各方面影响，</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>结论</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>区分性训练中的语言模型</td>
<td>unigram 优于 bigram 和 zerogram (无语言模型) 。</td>
<td>尽管在某些测试集上，zerogram 模型或者按比例缩小的 unigram 模型优于 unigram, 但是会增加常用词的错误。</td>
</tr>
<tr>
<td>重新生成 Lattice</td>
<td>在训练 MMI 的过程中不需要更新 Lattice。</td>
<td>尽管在 MMI 的训练过程中，基于最新模型重新生成 Lattice 或对齐有最多 0.3% 的绝对 WER 提升，但是由于较高的计算复杂度，所以并不推荐在训练过程中更新 Lattice或对齐。</td>
</tr>
<tr>
<td>Lattice 的大小</td>
<td>存在一个合适的阈值，使得在不大幅减小最终 MMI 估计识别精度的前提下，可以减小 Lattice 的复杂度。</td>
<td>若备选路径 <script type="math/tex">p^{'}</script> 满足 <script type="math/tex">\log \mathbb{P}(p^{'})/\max_{p}\{\log\mathbb{P}(p)\}<阈值</script>，则对该路径进行剪枝。实验结果表明，阈值取100 时，词错误率减小量不超过 0.2%。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2-4-Lattice-free-MMI"><a href="#2-4-Lattice-free-MMI" class="headerlink" title="2.4 Lattice-free MMI"></a>2.4 Lattice-free MMI</h3><h2 id="3-其他区分性准则"><a href="#3-其他区分性准则" class="headerlink" title="3. 其他区分性准则"></a>3. 其他区分性准则</h2><h3 id="3-1-scaled-MMI"><a href="#3-1-scaled-MMI" class="headerlink" title="3.1 scaled MMI"></a>3.1 scaled MMI</h3><p>在 MMI 的目标函数中，分子和分母部分的词序列概率通常采用简单的 unigram 语言模型来近似，因为 unigram 模型忽略了词与词之间的相关性，所以导致备选词序列集合中概率高的词序列数量较少。最终导致 MMI 的目标函数不需要怎么调整声学模型参数就可以达到较大值， 但是却没有提高声学模型对于混淆路径的区分度。为了解决混淆路径较少的问题，引入如下的 scaled MMI 目标函数，</p>
<script type="math/tex; mode=display">
\sum_{i=1}^N\log\frac{\mathbb{P}_{\theta}(O_{i}|S_i)^{\alpha}\mathbb{P}(S_i)^{\alpha}}{\sum_{j}\mathbb{P}_{\theta}(O_i|S_i^j)^{\alpha}\mathbb{P}(S_i^j)^{\alpha}}</script><p>其中 <script type="math/tex">\alpha</script> 被称为尺度因子 (scale factor)，<script type="math/tex">0<\alpha<1</script>。</p>
<p>加入尺度因子之后，能够缩小备选词序列集合中词序列之间的概率差异，从而增加混淆路径的数量。使得最大化 scaled MMI 目标函数能够得到更具有区分性的声学模型 <script type="math/tex">\mathbb{P}_{\theta}(O_i|S_i)</script>。</p>
<p><p align = "center"><br>    <img height="400px" src = "./images/scaleFactor.png"><br>    图 3-1 尺度因子对词序列概率的作用</p>
<p>为了直观说明尺度因子<script type="math/tex">\alpha</script> 的作用，图 3-1 给出了四个不同的尺度因子条件下词序列概率 <script type="math/tex">\mathbb{P}(S_i^j)^{\alpha}</script> 的变化。 假设 <script type="math/tex">\mathbb{P}(S_i^1)=0.9, \mathbb{P}(S_i^2)=0.8</script>，两条红色的水平虚线和纵轴的交点间距表示 <script type="math/tex">\alpha=1</script> 时两个词序列概率的差；而两条蓝色的水平虚线和纵轴的交点间距表示 <script type="math/tex">\alpha=0.25</script> 时两个词序列概率的差。从图 3-1 中可以看到，蓝色的交点间距明显小于红色的交点间距，所以应用更小的尺度因子可以减小词序列概率间的差异。</p>
<h3 id="3-2-BMMI"><a href="#3-2-BMMI" class="headerlink" title="3.2 BMMI"></a>3.2 BMMI</h3><p>增强的 MMI (BMMI, Boosted MMI) 目标函数为,</p>
<script type="math/tex; mode=display">
\sum_{i=1}^{N}\log\frac{\mathbb{P}_{\theta}(O_i|S_i)^{\alpha}\mathbb{P}(S_i)^{\alpha}}{\sum_{j}\mathbb{P}_{\theta}(O_i|S_i^j)^{\alpha}\mathbb{P}(S_i^{j})^{\alpha}\exp\{-b\cdot A(S_i^j,S_i)\}}</script><p>其中 <script type="math/tex">\alpha</script> 是尺度因子，b 是非负的增强因子 (boosting factor), <script type="math/tex">A(S_i^j, S_i)</script> 度量备选文本 <script type="math/tex">S_i^j</script> 和正确文本 $S_i$ 之间的匹配程度。常用已经训练好的 GMM-HMM 或者 DNN-HMM 把音频信号 <script type="math/tex">O_i</script> 和 <script type="math/tex">S_i</script> 以及 <script type="math/tex">S_i^j</script> 分别进行帧-音素级别的对齐，然后统计两个帧-音素对齐中结果相同的帧数。<script type="math/tex">S_i^j</script> 和 <script type="math/tex">S_i</script> 匹配程度越高， 该备选词序列的重要度越低；反之 <script type="math/tex">S_i^j</script> 和 <script type="math/tex">S_i</script> 匹配程度越低，该备选词序列的重要度越高。最大化 BMMI 目标函数， 匹配程度较低的备选词序列相对于匹配程度较高的备选词序列来说，声学模型 <script type="math/tex">\mathbb{P}_{\theta}(O_i|S_i^j)</script> 的取值会变小。这就使得经过序列区分训练后的声学模型更倾向于选择更贴近真实词序列的备选答案。</p>
<h3 id="3-3-MPE-MWE-sMBR"><a href="#3-3-MPE-MWE-sMBR" class="headerlink" title="3.3 MPE/MWE/sMBR"></a>3.3 MPE/MWE/sMBR</h3><p>BMMI 在 MMI 目标函数分母的每一项增加了权重，如果在 MMI 目标函数分子增加权重，修改目标函数如下，</p>
<script type="math/tex; mode=display">
\sum_{i=1}^{N}\frac{\sum_j\mathbb{P}_{\theta}(O_i|S_i^j)^{\alpha}\mathbb{P}(S_i^j)^{\alpha}\cdot A(S_i^j, S_i)}{\sum_{j}\mathbb{P}_{\theta}(O_i|S_i^j)^{\alpha}\mathbb{P}(S_i^j)^{\alpha}}</script><p>需要特别注意上面的目标函数是N个训练样本上概率求和，而不是取了对数之后再求和。如果上面公式中的 <script type="math/tex">A(S_i^j,S_i)</script> 和 BMMI 一样，都是度量正确文本 <script type="math/tex">S_i</script> 和备选文本 <script type="math/tex">S_i^j</script> 之间帧-音素级别的匹配程度，则该目标函数被称为 MPE (Minimum Phone Error) 准则。 </p>
<p>如果上面公式中的 <script type="math/tex">A(S_i^j, S_i)</script> 度量备选文本 <script type="math/tex">S_i</script> 和正确文本 <script type="math/tex">S_i^j</script> 对齐之后正确词 (字) 的个数，则该目标函数被称为 MWE (Minimum Word Error) 准则。文本之间的对齐可以参考编辑距离的计算，或者考虑 WER 的计算过程。</p>
<p>如果上面公式中的 <script type="math/tex">A(S_i^j, S_i)</script> 度量备选文本 <script type="math/tex">S_i</script> 和正确文本 <script type="math/tex">S_i^j</script> 在帧-状态级别的匹配程度，即利用训练好的 GMM-HMM 或者 DNN-HMM 对音频信号 <script type="math/tex">O_i</script> 和 <script type="math/tex">S_i</script> 以及 <script type="math/tex">S_i^j</script> 分别进行帧-状态级别的对齐，然后统计两个帧-状态对齐中结果相同的帧数， 则该目标函数被称为 sMBR (state-Minimum Bayesian Risk)。 </p>
<p>不论是 MPE，MWE 或者 sMBR， 都是给予了和正确文本相似的备选文本较高的权重，优化目标函数之后得到的声学模型 <script type="math/tex">\mathbb{P}_{\theta}(O_i|S_i^j)</script> 在相似度高的文本上给予较高的声学得分，而在相似度低的文本上给予较低的声学得分。</p>
<p>对比 MPE 和 MWE 的目标函数，理论上，当训练数据量趋于无穷， MWE 的目标函数应该逼近模型的泛化 WER， 所以最小化 MWE 的目标函数应该和最小化 WER 的目标更匹配。  但是，在目前能够得到的训练集上，实验结果表明 MPE 在测试集上的表现比 MWE 的更好一些 [Povey, 2003]。</p>
<h2 id="4-Kaldi-实现"><a href="#4-Kaldi-实现" class="headerlink" title="4. Kaldi 实现"></a>4. Kaldi 实现</h2><h2 id="5-本章小结"><a href="#5-本章小结" class="headerlink" title="5. 本章小结"></a>5. 本章小结</h2><h2 id="6-参考资料"><a href="#6-参考资料" class="headerlink" title="6. 参考资料"></a>6. 参考资料</h2><h2 id="7-附录"><a href="#7-附录" class="headerlink" title="7. 附录"></a>7. 附录</h2><h2 id="Text-Formatting"><a href="#Text-Formatting" class="headerlink" title="Text Formatting"></a>Text Formatting</h2><p>Regular, <strong>bold</strong>, <em>italic</em>, <del>strike</del>, ==hightlight==, <code>inline-code</code>,*emphasis^,<!--comment-->,</p>
<h2 id="Cites"><a href="#Cites" class="headerlink" title="Cites"></a>Cites</h2><blockquote>
<p>[Povey ]e</p>
</blockquote>
<h2 id="Code"><a href="#Code" class="headerlink" title="Code:"></a>Code:</h2><figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> someCode <span class="keyword">from</span> <span class="string">&#x27;someLibrary&#x27;</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h2><p><a href="www.google.com">This is a link</a></p>
<h2 id="Footnote"><a href="#Footnote" class="headerlink" title="Footnote"></a>Footnote</h2><p>Some thing </p>
<h2 id="Superscripts"><a href="#Superscripts" class="headerlink" title="Superscripts"></a>Superscripts</h2><p>Example^1^</p>
<p>Example~2~</p>
]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>语音识别</tag>
      </tags>
  </entry>
</search>
